version: '3'

services:
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    user: root
    ports:
      - "9083:9083"
      - "10000:10000"
    environment:
      - SERVICE_NAME=metastore
    volumes:
      - hive-data:/tmp
    command: >
      sh -c "
        mkdir -p /tmp/hive &&
        chmod -R 777 /tmp/hive &&
        /opt/hive/bin/schematool -dbType derby -initSchema &&
        /opt/hive/bin/hive --service metastore
      "

  spark-runner:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-runner
    depends_on:
      - hive-metastore
    volumes:
      - ./vehicle-etl.jar:/app/vehicle-etl.jar
      - ./init-data:/app/init-data
    command: sh -c 'echo "Waiting for Hive Metastore..." && sleep 30 && mkdir -p /app/init-data && echo "id,make,model,year,color,vin,manufacturer,gearPosition,timestamp" > /app/init-data/vehicles.csv && echo "1,Toyota,Camry,2020,Blue,ABC123XYZ456789,Toyota,D,1617235200000" >> /app/init-data/vehicles.csv && echo "2,Honda,Accord,2021,Black,DEF456UVW789012,Honda,P,1617321600000" >> /app/init-data/vehicles.csv && echo "3,Ford,F-150,2019,Red,GHI789RST012345,Ford,N,1617408000000" >> /app/init-data/vehicles.csv && /opt/spark/bin/spark-sql --master local[1] --conf spark.sql.catalogImplementation=hive --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 --conf spark.sql.warehouse.dir=/user/hive/warehouse -e "CREATE DATABASE IF NOT EXISTS vehicle_datalake; USE vehicle_datalake; CREATE TABLE IF NOT EXISTS events (id INT, make STRING, model STRING, year INT, color STRING, vin STRING, manufacturer STRING, gearPosition STRING, timestamp BIGINT) USING csv OPTIONS (path \"/app/init-data/vehicles.csv\", header \"true\"); CREATE TABLE IF NOT EXISTS processed_vehicles (id INT, make STRING, model STRING, year INT, color STRING, vin STRING, manufacturer STRING, gearPosition INT) PARTITIONED BY (date STRING, hour STRING); SHOW TABLES;" && echo "Tables initialized. Running ETL job..." && chmod 755 /app/vehicle-etl.jar && /opt/spark/bin/spark-submit --master local[1] --class com.insite.etl.common.ETLRunner --conf spark.sql.catalogImplementation=hive --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 --conf spark.sql.warehouse.dir=/user/hive/warehouse /app/vehicle-etl.jar com.insite.etl.vehicle.VehicleETL partition-columns=date,hour base-path=/tmp/vehicle-data target-table=vehicle_datalake.processed_vehicles version=v001'

volumes:
  hive-data:

networks:
  default:
    name: etl-network

