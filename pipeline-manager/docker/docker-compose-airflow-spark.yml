services:
  # Spark and Hive services
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    user: root
    ports:
      - "9083:9083"
      - "10000:10000"
    environment:
      - SERVICE_NAME=metastore
    volumes:
      - hive-data:/tmp
    command: >
      sh -c "
        mkdir -p /tmp/hive &&
        chmod -R 777 /tmp/hive &&
        /opt/hive/bin/schematool -dbType derby -initSchema &&
        /opt/hive/bin/hive --service metastore
      "

  spark-runner:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-runner
    depends_on:
      - hive-metastore
    volumes:
      - ./dags:/app/dags
      - ./jars:/app/jars
      - ./sql:/app/sql
      - ${PIPELINE_DATA_PATH}:/app/data
    command: >
      bash -c "echo 'This is a placeholder. Real execution happens via pipeline manager.'; sleep infinity"

  # Airflow services
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  airflow-init:
    image: apache/airflow:2.7.1
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=gMvpWhmVdQsGV-NxDdQ9bZeYEEQ7bd4E6qX1HHVnOYg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
        echo 'Waiting for Postgres...';
        while ! pg_isready -h postgres -U airflow; do
        sleep 2;
        done;
        echo 'Postgres is ready.';
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin;
        echo 'Airflow initialized. Exiting.';
      "

  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8081:8080"
    command: bash -c "airflow db check && airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-runner:7077
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler

volumes:
  hive-data:
  postgres-data:
  airflow-logs:

networks:
  default:
    name: etl-network

